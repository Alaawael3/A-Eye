{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:08:56.357624Z",
     "iopub.status.busy": "2025-04-24T12:08:56.357023Z",
     "iopub.status.idle": "2025-04-24T12:09:16.568966Z",
     "shell.execute_reply": "2025-04-24T12:09:16.567531Z",
     "shell.execute_reply.started": "2025-04-24T12:08:56.357588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from tensorflow.keras.applications import InceptionV3,DenseNet201\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical,plot_model\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,Embedding,Dropout,add,BatchNormalization,Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler,ReduceLROnPlateau,ModelCheckpoint\n",
    "import glob as glob\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:24.312858Z",
     "iopub.status.busy": "2025-04-24T12:09:24.312449Z",
     "iopub.status.idle": "2025-04-24T12:09:24.997146Z",
     "shell.execute_reply": "2025-04-24T12:09:24.995996Z",
     "shell.execute_reply.started": "2025-04-24T12:09:24.312803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_data)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(caption.split()) for caption in cleaned_data)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:24.998688Z",
     "iopub.status.busy": "2025-04-24T12:09:24.998305Z",
     "iopub.status.idle": "2025-04-24T12:09:25.010926Z",
     "shell.execute_reply": "2025-04-24T12:09:25.009656Z",
     "shell.execute_reply.started": "2025-04-24T12:09:24.998649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images=data['image'].unique().tolist()\n",
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:25.012505Z",
     "iopub.status.busy": "2025-04-24T12:09:25.012093Z",
     "iopub.status.idle": "2025-04-24T12:09:25.026302Z",
     "shell.execute_reply": "2025-04-24T12:09:25.025138Z",
     "shell.execute_reply.started": "2025-04-24T12:09:25.012472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "split_index = round(0.85*len(all_images))\n",
    "\n",
    "train_images=all_images[:split_index]\n",
    "val_images=all_images[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:25.028040Z",
     "iopub.status.busy": "2025-04-24T12:09:25.027641Z",
     "iopub.status.idle": "2025-04-24T12:09:25.059065Z",
     "shell.execute_reply": "2025-04-24T12:09:25.057518Z",
     "shell.execute_reply.started": "2025-04-24T12:09:25.028007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train=data[data['image'].isin(train_images)]\n",
    "val=data[data['image'].isin(val_images)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Image features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:25.164255Z",
     "iopub.status.busy": "2025-04-24T12:09:25.163745Z",
     "iopub.status.idle": "2025-04-24T12:09:31.293971Z",
     "shell.execute_reply": "2025-04-24T12:09:31.292996Z",
     "shell.execute_reply.started": "2025-04-24T12:09:25.164209Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m82524592/82524592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "dense_model = DenseNet201()\n",
    "dense_model = Model(inputs=dense_model.inputs, outputs=dense_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:31.295541Z",
     "iopub.status.busy": "2025-04-24T12:09:31.295111Z",
     "iopub.status.idle": "2025-04-24T12:09:31.300033Z",
     "shell.execute_reply": "2025-04-24T12:09:31.298819Z",
     "shell.execute_reply.started": "2025-04-24T12:09:31.295503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features = {}\n",
    "for image in tqdm(data['image'].unique().tolist()):\n",
    "    img = load_img(os.path.join(img_path, image), target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    feature = dense_model.predict(img, verbose=0)\n",
    "    features[image] = feature  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:32.095250Z",
     "iopub.status.busy": "2025-04-24T12:09:32.094994Z",
     "iopub.status.idle": "2025-04-24T12:09:32.105310Z",
     "shell.execute_reply": "2025-04-24T12:09:32.104127Z",
     "shell.execute_reply.started": "2025-04-24T12:09:32.095228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class datasetgenerator(Sequence):\n",
    "    def __init__(self,data,x_col,y_col,batch_size,tokenizer,vocab_size,max_length,features,shuffle=True):\n",
    "        self.data = data.copy()\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.features = features\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.data)\n",
    "\n",
    "    def on_epoch(self): #Shuffles the data after each epoch \n",
    "        if self.shufle:\n",
    "            self.data=self.data.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        batch=self.data.iloc[index*self.batch_size:(index+1)*self.batch_size,:]\n",
    "        x1,x2,y=self.__get_data(batch) #X1: Image features X2: Input sequence (caption so far) y: Target word (next word in the caption)\n",
    "        return (x1,x2),y\n",
    "\n",
    "    def __get_data(self,batch):\n",
    "        x1,x2,y=[],[],[]\n",
    "        images=batch[self.x_col].tolist() #Extracted image names\n",
    "        \n",
    "        for img in images:\n",
    "            feature=self.features[img][0] #features of the image\n",
    "            captions=batch.loc[batch[self.x_col]==img,self.y_col].tolist() #Captions for the image\n",
    "            \n",
    "            for caption in captions:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(len(seq)):\n",
    "                    in_seq,out_seq= seq[:i],seq[i]\n",
    "                    in_seq= pad_sequences([in_seq],maxlen=self.max_length,)[0]\n",
    "                    out_seq= to_categorical([out_seq],num_classes=self.vocab_size)[0]\n",
    "                    x1.append(feature)\n",
    "                    x2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "        return np.array(x1), np.array(x2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:32.106882Z",
     "iopub.status.busy": "2025-04-24T12:09:32.106532Z",
     "iopub.status.idle": "2025-04-24T12:09:32.134269Z",
     "shell.execute_reply": "2025-04-24T12:09:32.133011Z",
     "shell.execute_reply.started": "2025-04-24T12:09:32.106854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_data_generator=datasetgenerator(train,'image','caption',batch_size,tokenizer,vocab_size,max_length,features)\n",
    "val_data_generator=datasetgenerator(val,'image','caption',batch_size,tokenizer,vocab_size,max_length,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:09:32.900160Z",
     "iopub.status.busy": "2025-04-24T12:09:32.899885Z",
     "iopub.status.idle": "2025-04-24T12:17:18.094919Z",
     "shell.execute_reply": "2025-04-24T12:17:18.093886Z",
     "shell.execute_reply.started": "2025-04-24T12:09:32.900138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load FastText vectors (replace path with the actual .vec or .bin file path)\n",
    "fasttext = KeyedVectors.load_word2vec_format('/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:17:18.096496Z",
     "iopub.status.busy": "2025-04-24T12:17:18.096064Z",
     "iopub.status.idle": "2025-04-24T12:17:18.146529Z",
     "shell.execute_reply": "2025-04-24T12:17:18.145312Z",
     "shell.execute_reply.started": "2025-04-24T12:17:18.096451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300  # because FastText vectors are 300-dimensional\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in fasttext:\n",
    "        embedding_matrix[i] = fasttext[word]\n",
    "    else:\n",
    "        # For unknown words, you can leave them as zeros or use a random vector\n",
    "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:17:18.147884Z",
     "iopub.status.busy": "2025-04-24T12:17:18.147562Z",
     "iopub.status.idle": "2025-04-24T12:17:18.185170Z",
     "shell.execute_reply": "2025-04-24T12:17:18.183836Z",
     "shell.execute_reply.started": "2025-04-24T12:17:18.147855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, max_length, embedding_matrix, embedding_dim=300):\n",
    "    # Encoder\n",
    "    Input_image = Input(shape=(1920,), name='Feature_Input')  # Flattened shape\n",
    "    x = BatchNormalization()(Input_image)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Decoder\n",
    "    Input_cation = Input(shape=(max_length,), name='Caption_Input')\n",
    "    y = Embedding(input_dim=vocab_size, \n",
    "              output_dim=embedding_dim, \n",
    "              weights=[embedding_matrix], \n",
    "              input_length=max_length,\n",
    "              trainable=True)(Input_cation)  # Set trainable=True to fine-tune FastText\n",
    "\n",
    "    y = LSTM(256,return_sequences=True,)(y)\n",
    "    y = LSTM(256)(y)\n",
    "    \n",
    "    # Output\n",
    "    decoder = add([x, y])\n",
    "    decoder = Dense(256, activation='relu')(decoder)\n",
    "    output = Dense(vocab_size, activation='softmax', name='output_layer')(decoder)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[Input_image, Input_cation], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:17:18.186652Z",
     "iopub.status.busy": "2025-04-24T12:17:18.186243Z",
     "iopub.status.idle": "2025-04-24T12:17:18.475149Z",
     "shell.execute_reply": "2025-04-24T12:17:18.474228Z",
     "shell.execute_reply.started": "2025-04-24T12:17:18.186610Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning:\n",
      "\n",
      "Argument `input_length` is deprecated. Just remove it.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Feature_Input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1920</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ Caption_Input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ batch_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1920</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">7,680</span> │ Feature_Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,643,900</span> │ Caption_Input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">491,776</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">570,368</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ batch_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1… │\n",
       "│                           │                        │                │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8813</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,264,941</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Feature_Input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1920\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ Caption_Input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ batch_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1920\u001b[0m)           │          \u001b[38;5;34m7,680\u001b[0m │ Feature_Input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │      \u001b[38;5;34m2,643,900\u001b[0m │ Caption_Input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m491,776\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m570,368\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ batch_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │          \u001b[38;5;34m1,024\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m525,312\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ batch_normalization_1… │\n",
       "│                           │                        │                │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m65,792\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8813\u001b[0m)           │      \u001b[38;5;34m2,264,941\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,570,793</span> (25.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,570,793\u001b[0m (25.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,566,441</span> (25.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,566,441\u001b[0m (25.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> (17.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,352\u001b[0m (17.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(vocab_size, max_length, embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:17:18.887940Z",
     "iopub.status.busy": "2025-04-24T12:17:18.887617Z",
     "iopub.status.idle": "2025-04-24T12:17:18.906775Z",
     "shell.execute_reply": "2025-04-24T12:17:18.905516Z",
     "shell.execute_reply.started": "2025-04-24T12:17:18.887913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4) #weight_decay: A technique that penalizes large weights by adding an L2 penalty term to the loss.To prevent overfitting by keeping the model weights small and simple.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:17:18.908502Z",
     "iopub.status.busy": "2025-04-24T12:17:18.908146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning:\n",
      "\n",
      "Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - loss: 4.9335 \n",
      "Epoch 1: val_loss improved from inf to 3.48649, saving model to model.keras\n",
      "\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6802s\u001b[0m 13s/step - loss: 4.9321 - val_loss: 3.4865 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - loss: 3.2192 \n",
      "Epoch 2: val_loss improved from 3.48649 to 3.20484, saving model to model.keras\n",
      "\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6765s\u001b[0m 13s/step - loss: 3.2191 - val_loss: 3.2048 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - loss: 2.7879 \n",
      "Epoch 3: val_loss improved from 3.20484 to 3.10017, saving model to model.keras\n",
      "\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6750s\u001b[0m 13s/step - loss: 2.7879 - val_loss: 3.1002 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - loss: 2.4812 \n",
      "Epoch 4: val_loss did not improve from 3.10017\n",
      "\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6737s\u001b[0m 13s/step - loss: 2.4812 - val_loss: 3.1320 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m361/537\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34:27\u001b[0m 12s/step - loss: 2.2239"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "lr_schedul=ReduceLROnPlateau(monitor='val_loss', \n",
    "                             patience=3, \n",
    "                             verbose=1, \n",
    "                             factor=0.2, \n",
    "                             min_lr=0.00000001)\n",
    "model_name = \"model.keras\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only = True,\n",
    "                            verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_generator,\n",
    "    validation_data=val_data_generator,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping,lr_schedul,checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], color='green', linestyle='-', marker='o', markersize=5, label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], color='red', linestyle='--', marker='x', markersize=5, label='Validation Loss')\n",
    "\n",
    "plt.title('Model Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.legend(loc='upper left', fontsize=12)\n",
    "\n",
    "plt.xlim(0, len(history.history['loss']) - 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 14154,
     "sourceId": 19053,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6701767,
     "sourceId": 10798078,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6701807,
     "sourceId": 10798130,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6877788,
     "sourceId": 11041516,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7125727,
     "sourceId": 11380607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7165870,
     "sourceId": 11439481,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
